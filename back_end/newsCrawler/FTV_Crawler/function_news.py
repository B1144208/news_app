import re
import os
import json
import time
import random

from typing import Optional
from urllib.parse import urljoin
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

from newsCrawler import utils
from . import function_channel


from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from fake_useragent import UserAgent

def change_fake_ua(driver):

    driver.quit()

    # åˆå§‹åŒ– Chrome æµè§ˆå™¨é€‰é¡¹
    options = Options()
    
    # ä½¿ç”¨ fake_useragent ç”Ÿæˆéšæœºçš„ User-Agent
    ua = UserAgent()
    user_agent = ua.random  # éšæœºç”Ÿæˆä¸€ä¸ª User-Agent
    
    # è®¾ç½®éšæœºçš„ User-Agent
    options.add_argument(f"user-agent={user_agent}")
    
    # åˆ›å»º WebDriver å®ä¾‹
    driver = webdriver.Chrome(options=options)
    
    # åŠ¨æ€ä¿®æ”¹ User-Agentï¼ˆå¯é€‰ï¼‰
    driver.execute_cdp_cmd('Network.setUserAgentOverride', {
        "userAgent": user_agent  # ä½¿ç”¨åˆšæ‰ç”Ÿæˆçš„éšæœº User-Agent
    })
    
    print("Driver initialized with User-Agent:", user_agent)
    return driver

# å®šç¾©å‡½å¼
"""
æµç¨‹å‡½å¼: start_news_collection -> extract_news_urls -> get_news_information
""" 
def start_news_collection(BASE_URL, driver):
    """ TODO: æ“·å–å…¨éƒ¨çš„ GROUP èˆ‡ SUB_URL å‚³çµ¦ extract_news_urls() ( ä¸»é å…¥å£è™• ) """
    """       å‘¼å«å¾Œæœ€å¾Œæœƒç²å¾—å…¨éƒ¨çš„ news_information """
    ...
def extract_news_urls(BASE_URL, SUB_URL, GROUP, driver):
    """ TODO: æ“·å– SUB_URL çš„å…¨éƒ¨ NEWS_URL å‚³çµ¦ get_news_information() """
    """       æ“·å–ä¸€å€‹ SUB_URL å°±é¦¬ä¸Šå‚³çµ¦ get_news_information() """
    ...
def get_news_information(NEWS_URLS: list[str], driver: WebDriver, GROUP: Optional[str] = None) -> list[dict]:
    """ TODO: æ“·å– NEWS_URL çš„å…¨éƒ¨æ–°èè³‡è¨Šï¼Œä¸¦å­˜æˆ JSON æ ¼å¼çš„ list """
    ...


# ==== ä»¥ä¸‹ç‚ºå¯¦ä½œç´°ç¯€ ====
# æ“·å–å„åˆ†é¡é€£çµ (æ–°èæ“·å–èµ·é»)
def start_news_collection(BASE_URL, driver):

    print("ğŸš€ é–‹å§‹è¼‰å…¥é¦–é ï¼š", BASE_URL)
    driver.get(BASE_URL)
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, "menuNav"))
    )

    # ä½¿ç”¨ BeautifulSoup æ“·å– HTML
    soup = BeautifulSoup(driver.page_source, "html.parser")
    # utils.save_html_source(str(soup))

    # æ“·å–åˆ†é¡é€£çµ
    menu_nav = soup.find("div", id="menuNav")
    menu_items = menu_nav.select("ul.MenuList a") if menu_nav else []
    
    previous_pref = None

    for li in menu_items:
        href = li.get("href")
        group = li.get_text(strip=True)

        if not href:
            continue    # å¿½ç•¥ href ç‚ºç©º

        # æ¨™æº–åŒ–ç¶²å€
        href = utils.normalize_url(BASE_URL, href)
        
        if(href==previous_pref):
            continue
        
        previous_pref = href
        extract_news_urls(BASE_URL, href, group, driver)

    return

# æ“·å–åˆ†é¡ä¹‹æ–°èé€£çµ
def extract_news_urls(BASE_URL, SUB_URL, GROUP, driver):
    
    if "author" in SUB_URL:
        return
    if GROUP in ["é¦–é ", "å³æ™‚", "é«”è‚²", "è²¡ç¶“", "é•·ç…§", "è‹±èªæ–°è", "æ•¸ä½å°ˆé¡Œ"]:
        return
    if GROUP in ["å³æ™‚"]:
        return

    driver.get(SUB_URL)
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "news-block"))
    )

    soup = BeautifulSoup(driver.page_source, "html.parser")
    # utils.save_html_source(str(soup), "b_temp.html")

    print(f"é–‹å§‹æ“·å–{GROUP}åˆ†é¡:", BASE_URL)
    
    
    # æ“·å–ç¸½é æ•¸
    page_info = soup.find("span", class_="pagiNum")
    if not page_info or "/" not in page_info.text:
        print(f"âŒ : <{GROUP}åˆ†é¡> ç„¡æ³•å–å¾—ç¸½é æ•¸")
        return []
    try:
        _, total_page = page_info.text.strip().split(":")
        _, total_page = map(int, total_page.strip().split("/"))
    except Exception as e:
        print(f"âŒ : <{GROUP}åˆ†é¡> ç¸½é æ•¸è§£æéŒ¯èª¤")
    
    
    for page in range(1, total_page + 1):

        page_url = f"{SUB_URL}/{str(page)}"
        #driver = change_fake_ua(driver)
        driver.get(page_url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "pagiNum"))
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # æ“·å–æ–°èé€£çµ
        news_urls = []
        news_block = soup.find_all("div", class_="news-block")
        for block in news_block:
            href = block.find("a", class_ = "img-block").get("href")
            
            if not href:
                continue

            # æ¨™æº–åŒ–ç¶²å€
            href = utils.normalize_url(BASE_URL, href)

            news_urls.append(href)
        if news_urls:
            get_news_information(news_urls, driver, GROUP=GROUP)

        time.sleep(random.uniform(2, 4))
        print(f"âœ…: æ“·å–å®Œæˆ {GROUP}åˆ†é¡ : ç¬¬ {page} é ")
    
    print(f"<{GROUP}é¡åˆ¥> æ–°èæ“·å–å®Œæˆ")
    return


# æ“·å–æ–°èå®Œæ•´è³‡è¨Š
def get_news_information(NEWS_URLS: list[str], driver: WebDriver, GROUP: Optional[str] = None, CHANNEL: Optional[str] = None) -> list[dict]:
    
    # ç²å–å·²æœ‰çš„ news_url
    news_data = utils.load_json("NEWS_DATA_bella.json")
    existing_urls = {item["url"] for item in news_data if "url" in item}
    data = []
    for url in NEWS_URLS:

        # å¦‚æœå·²ç¶“æœ‰è©²æ–°èï¼Œå‰‡è·³é
        if url in existing_urls:
            continue

        article = {}
        driver.get(url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "text-center"))
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # å­˜å–æ–°èåŸå§‹é€£çµ (ç”¨ä¾†åˆ¤æ–·æ˜¯å¦å·²ç¶“æœ‰è©²æ–°è)
        article["url"] = url

        # å­˜å–æ–°èåˆ†é¡ ( å¯ None )
        article["group"] = GROUP if GROUP else None
        
        # æ“·å–æ–°èå°è³‡è¨Š
        if not CHANNEL:
            # å¦‚æœæœ‰ channel è³‡è¨Š
            channel_name = "æ°‘è¦–æ–°èç¶²"  # é è¨­å€¼ç‚º "æ°‘è¦–æ–°èç¶²"
            channel = soup.find("div", class_="author-detail")
            if channel:
                channel_name = channel.find("div", class_="name").text
                channel_job = channel.find("div", class_="job").text

                # ä½¿ç”¨ classify_author ä¾†åˆ¤æ–· tag
                tag, _ = function_channel.classify_author(channel_name, channel_job)
                if tag=="people":
                    channel_name = "æ°‘è¦–æ–°èç¶²"
    
            article["channel"] = channel_name
        else:
            article["channel"] = CHANNEL

        # åœ–ç‰‡(å°é¢)
        try:
            cover_img = driver.find_element(By.CSS_SELECTOR, "#View_Img img")
            cover_img_src = cover_img.get_attribute("src")
            # å˜—è©¦æŠ“å– alt å±¬æ€§ï¼Œè‹¥æ‰¾ä¸åˆ°å‰‡è¨­ç‚º None
            try:
                cover_img_alt = driver.find_element(By.CSS_SELECTOR, "#View_Img figcaption").text
            except Exception as e:
                cover_img_alt = None
            
            article["cover_img"] = {
                "src": cover_img_src,
                "alt": cover_img_alt
            }
        except Exception as e:
            article["cover_img"] = None
            print("âŒ å°é¢åœ–ç‰‡æ“·å–å¤±æ•—ï¼š", e)

        # æ¨™é¡Œ
        news_title = soup.find("h1", class_="text-center border-0").get_text(strip=True)
        article["news_title"] = news_title

        # ç™¼å¸ƒæ™‚é–“
        publish_date = soup.find("span", class_="date").get_text(strip=True)
        publish_date = publish_date.split("ï¼š")[1]
        article["publish_date"] = publish_date
        
        # å…§æ–‡( text + img )
        detail = []
        
        # æŠ“å– preface å€å¡Š
        preface = soup.find("div", id="preface")
        p_tags = preface.find_all("p")
        if p_tags:
            for p in p_tags:
                content_html = str(p)
                # å¦‚æœåŒ…å« <br> ï¼Œå°±æ‹†é–‹
                if "<br" in content_html:
                    segments = re.split(r"<\s*br\s*/\s*>", p.decode_contents(), flags = re.IGNORECASE)
                    for seg in segments:
                        clean_text = BeautifulSoup(seg, "html.parser").get_text(strip=True)
                        if clean_text:
                            detail.append({"text": clean_text})
                # å¦‚æœä¸åŒ…å« <br> ï¼Œæ•´æ®µè™•ç†
                else:
                    text = p.get_text(strip=True)
                    if text:
                        detail.append({"text": text})

        # æ–°èå…§æ–‡ (text + img)
        content = soup.find("div", id="newscontent")
        for element in content.find_all(["p", "figure"]):
            if element.name=="p":
                content_html = str(element)
                # å¦‚æœåŒ…å« <br> ï¼Œå°±æ‹†é–‹
                if "<br" in content_html:
                    segments = re.split(r"<\s*br\s*/\s*>", element.decode_contents(), flags = re.IGNORECASE)
                    for seg in segments:
                        clean_text = BeautifulSoup(seg, "html.parser").get_text(strip=True)
                        if clean_text:
                            detail.append({"text": clean_text})
                # å¦‚æœä¸åŒ…å« <br> ï¼Œæ•´æ®µè™•ç†
                else:
                    text = element.get_text(strip=True)
                    if text:
                        detail.append({"text": text})

            elif element.name=="figure" and "fr-img-wrap" in element.get('class', []):
                img_tag = element.find('img')
                if img_tag and img_tag.get('src'):
                    # å¦‚æœæœ‰åœ–ç‰‡ï¼Œå‰‡ç²å–åœ–ç‰‡çš„ src
                    img_src = img_tag['src']
                    
                    # æŸ¥æ‰¾åœ–ç‰‡èªªæ˜ <figcaption>
                    figcaption = element.find('figcaption')
                    caption_text = figcaption.get_text(strip=True) if figcaption else None
                    
                    # å°‡åœ–ç‰‡çš„ src å’Œèªªæ˜ä¸€èµ·å„²å­˜
                    detail.append({
                        "img": {
                            'src':img_src, 
                            'alt': caption_text
                        }
                    })
        article["detail"] = detail

        
        # é—œéµå­—
        keyword = []
        news_tag = soup.find("div", class_="news-tag")
        news_tag = news_tag.find_all("li")
        for ky in news_tag:
            key = ky.find("a").text.strip()
            keyword.append(key)

        # å»é™¤é‡è¤‡èˆ‡ç©ºå€¼
        keyword = list(set(keyword))

        data.append(article)

    utils.save_data_to_json(data, output_file="NEWS_DATA_bella.json")
    return data



