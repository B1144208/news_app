
import json
import os
import time
import random

from urllib.parse import urljoin
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from fake_useragent import UserAgent

from . import CLASSIFY_KEYWORD
from . import function_news
from newsCrawler import utils

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
AUTHOR_CLASSIFY_PATH = os.path.join(BASE_DIR, "AUTHOR_CLASSIFY.json")
AUTHOR_UNKNOWN_PATH  = os.path.join(BASE_DIR, "AUTHOR_UNKNOWN.json" )

def change_fake_ua(driver):

    driver.quit()

    # åˆå§‹åŒ– Chrome æµè§ˆå™¨é€‰é¡¹
    options = Options()
    
    # ä½¿ç”¨ fake_useragent ç”Ÿæˆéšæœºçš„ User-Agent
    ua = UserAgent()
    user_agent = ua.random  # éšæœºç”Ÿæˆä¸€ä¸ª User-Agent
    
    # è®¾ç½®éšæœºçš„ User-Agent
    options.add_argument(f"user-agent={user_agent}")
    
    # åˆ›å»º WebDriver å®ä¾‹
    driver = webdriver.Chrome(options=options)
    
    # åŠ¨æ€ä¿®æ”¹ User-Agentï¼ˆå¯é€‰ï¼‰
    driver.execute_cdp_cmd('Network.setUserAgentOverride', {
        "userAgent": user_agent  # ä½¿ç”¨åˆšæ‰ç”Ÿæˆçš„éšæœº User-Agent
    })
    
    print("Driver initialized with User-Agent:", user_agent)
    return driver




# å®£å‘Šå‡½å¼
""" æµç¨‹å‡½å¼: start_channel_collection -> get_channel_information -> get_news_information """
def start_channel_collection(BASE_URL: str, SUB_TAG: str, driver: WebDriver) -> list[dict]:
    """ TODO: æ“·å–å…¨éƒ¨çš„ authors ( å°ˆæ¬„ä½œè€…å…¥å£è™• ) """
    ...
def get_channel_information(CHANNELS_URL: list[dict], driver: WebDriver) -> list[dict]:
    """ TODO: æ“·å– NEWS_URL çš„å…¨éƒ¨æ–°èè³‡è¨Šï¼Œä¸¦å­˜æˆ JSON æ ¼å¼çš„ list """
    ...

""" åŠŸèƒ½å‡½å¼ """
def load_author_json(file_path: str = AUTHOR_CLASSIFY_PATH) -> list[dict]:
    """ TODO: è¼‰å…¥ JSON æª”æ¡ˆ ( AUTHOR_CLASSIFY.json & AUTHOR_UNKNOWN.json ) """
    ...
def classify_author(name: str, job: str) -> tuple[str, bool]:
    """ TODO: åˆ†è¾¨ author æ˜¯ people / channel ï¼Œéƒ½ä¸æ˜¯å›å‚³ unknown """
    """       str åŒ…å« { people, channel, unknown}                """
    """       bool è¡¨ç¤ºæ˜¯å¦åœ¨ AUTHOR_CLASSIFY.json ä¸­              """
    ...
def save_author_data(authors: list[dict], classify_file: str="AUTHOR_CLASSIFY.json", unknown_file: str="AUTHOR_UNKNOWN.json"):
    """ TODO: å°‡é AUTHOR_CLASSIFY.json çš„ list å¯«å…¥ AUTHOR_CLASSIFY.json & AUTHOR_UNKNOWN.json ä¸­ """
    ...
def AUTHOR_UNKNOWN_save_to_AUTHOR_CLASSIFY():
    """ TODO: å°‡ AUTHOR_UNKNOWN.json ä¸­ tag ä¸ç‚º unknown çš„è³‡æ–™å¯«å…¥ AUTHOR_CLASSIFY.json ä¸­ """
    ...

# ==== ä»¥ä¸‹ç‚ºå¯¦ä½œç´°ç¯€ ====
# æ“·å–å„ä½œè€…é€£çµ (æ–°èå°æ“·å–èµ·é»)
def start_channel_collection(BASE_URL: str, SUB_TAG: str, driver: WebDriver) -> list[dict]:

    FULL_URL = urljoin(BASE_URL, SUB_TAG)
    print("ğŸš€ é–‹å§‹è¼‰å…¥å°ˆæ¬„ä½œè€…ï¼š", FULL_URL)

    
    driver.get(FULL_URL)
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "pagiNum"))
    )
    soup = BeautifulSoup(driver.page_source, "html.parser")


    # æ“·å–ç¸½é æ•¸
    page_info = soup.find("span", class_="pagiNum")
    if not page_info or "/" not in page_info.text:
        print("âŒ : <FTVå°ˆæ¬„ä½œè€…> ç„¡æ³•å–å¾—ç¸½é æ•¸")
        return []
    
    try:
        _, total_page = page_info.text.strip().split(":")
        _, total_page = map(int, total_page.strip().split("/"))
    except Exception as e:
        print("âŒ : <FTVå°ˆæ¬„ä½œè€…> ç¸½é æ•¸è§£æéŒ¯èª¤")
    
    author_url_list = []
    not_in_json = []

    for page in range(1, total_page+1):
        page_url = urljoin(BASE_URL, f"{SUB_TAG}/{str(page)}")

        
        driver.get(page_url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "pagiNum"))
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # æ“·å–ä½œè€…æ¸…å–®
        author_list = soup.find("div", class_="author-list")
        authors = author_list.find_all("li", class_="col-md-3 col-sm-4 col-6")
        # print(f"ğŸ“„ ç¬¬ {page} é ï¼Œå…± {total_page} é ï¼Œæ‰¾åˆ° {len(authors)} ä½ä½œè€…")

        for author in authors:
            a_tag = author.find("a", class_="author")
            href = a_tag.get("href", "")
            href = urljoin(BASE_URL, href)  # fr"{BASE_URL}\{href}"
            name = a_tag.find("div", class_="name").get_text(strip=True)
            job = a_tag.find("div", class_="job").get_text(strip=True)

            tag, is_new = classify_author(name, job)

            # åŠ å…¥ URL å›å‚³åˆ—è¡¨
            author_url_list.append({"href": href, "tag": tag})

            # åŠ å…¥æ–°è³‡æ–™åˆ° not_in_json
            if is_new:
                not_in_json.append({"name": name, "tag": tag})

        time.sleep(random.uniform(2, 4))

    # çµ±ä¸€å„²å­˜æ–°è³‡æ–™
    if not_in_json:
        save_author_data(not_in_json)
    
    #print(author_url_list)                # ç¶²å€æ ¼å¼æœ‰å•é¡Œ
    get_channel_information(BASE_URL, author_url_list, driver)

    return author_url_list

# æ“·å– channel è³‡è¨Š
def get_channel_information(BASE_URL: str, CHANNELS_URL: list[dict], driver: WebDriver) -> list[dict]:

    
    channels_data = utils.load_json("CHANNEL_DATA_bella.json")
    existing_urls = {item["url"] for item in channels_data if "url" in item}

    #driver = change_fake_ua(driver)
    channels = []
    for channel in CHANNELS_URL:
        
        

        news_url = []
        
        # æ“·å– channel è³‡æ–™
        channel_data = {}

        href = channel['href']
        tag = channel['tag']
        
        # å¦‚æœå·²ç¶“æœ‰è©²æ–°èï¼Œå‰‡è·³é
        if href in existing_urls:
            continue

        
        driver.get(href)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "pagiNum"))
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")

        channel = soup.find("div", class_="author")
        img_block = channel.find("div", class_="img-block")
        img = img_block.find("img", {"loading": "lazy"})["src"]
        name = channel.find("h1", class_="name").text.strip()
        job = channel.find("div", class_="job").text.strip()
        introduce = channel.find("div", class_="intro").text.strip()

        if tag=='channel':
            channel_data['url'] = href
            channel_data['img'] = img
            channel_data['name'] = name
            channel_data['type'] = job
            channel_data['update_rate'] = None
            channel_data['introduce'] = introduce

            channels.append(channel_data)
            #print(f"channel_data:\n{channel_data}\n\n")
        else:
            name = "æ°‘è¦–æ–°èç¶²"
            continue
        
        
        time.sleep(random.uniform(1, 3))

        #print(f"channels:\n{channels}\n\n")

        
        
        utils.save_data_to_json(channels, output_file="CHANNEL_DATA_bella.json")
        
        
        
        """
        # æ“·å–ç¸½é æ•¸
        page_info = soup.find("span", class_="pagiNum")
        if not page_info or "/" not in page_info.text:
            print(f"âŒ : <FTVå°ˆæ¬„ä½œè€…> {name} ç„¡æ³•å–å¾—ç¸½é æ•¸")
            return []
        
        try:
            _, total_page = page_info.text.strip().split(":")
            _, total_page = map(int, total_page.strip().split("/"))
        except Exception as e:
            print(f"âŒ : <FTVå°ˆæ¬„ä½œè€…> {name} ç¸½é æ•¸è§£æéŒ¯èª¤")

        # æ“·å–æ–°èé€£çµ
        for page in range(1, total_page + 1):
            page_url = f"{href}/{str(page)}"

            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "pagiNum"))
            )
            soup = BeautifulSoup(driver.page_source, "html.parser")

            news_list = soup.find("section", class_="news-list mt-30")  # å°‹æ‰¾æ‰€æœ‰ <a> æ¨™ç±¤ä¸¦ç²å– href å±¬æ€§
            news_list = soup.find_all("li", class_="col-md-4 col-sm-6")

            # å–å¾—æ¯å€‹ <a> æ¨™ç±¤çš„ href å±¬æ€§
            for news in news_list:
                a_tag = news.find("a", href=True)
                if(a_tag):
                    url = urljoin(BASE_URL, a_tag.get("href"))
                    news_url.append(url)

            time.sleep(random.uniform(2, 4))

        # print(f"news_url_list:\n[\n{news_url}\n]\n")
        function_news.get_news_information(news_url, driver, CHANNEL=name)
        """
    utils.save_data_to_json(channels, output_file="CHANNEL_DATA_bella.json")
    
    return channels

# è¼‰å…¥ json æª”æ¡ˆ
def load_author_json(file_path: str = AUTHOR_CLASSIFY_PATH) -> list[dict]:
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                return []
    return []

# åˆ¤åˆ¥é—œéµå­—æ˜¯äººé‚„æ˜¯æ–°èå°
def classify_author(name: str, job: str) -> tuple[str, bool]:
    job_text = f"{name}{job}"

    # åˆ¤æ–· name æ˜¯å¦åœ¨ AUTHOR_CLASSIFY.json ä¸­
    classify_data = load_author_json()
    for author in classify_data:
        if author.get("name") == name:
            return (author.get("tag", "unknown"), False)

    if any( keyword in job_text for keyword in CLASSIFY_KEYWORD.CHANNEL_KEYWORDS ):
        return ("channel", True)
    elif any( keyword in job_text for keyword in CLASSIFY_KEYWORD.PEOPLE_KEYWORDS ):
        return ("people", True)
    else:
        return ("unknown", True)
    
# å„²å­˜ author è³‡æ–™
def save_author_data(authors: list[dict], classify_file: str="AUTHOR_CLASSIFY.json", unknown_file: str="AUTHOR_UNKNOWN.json"):
    """
    å„²å­˜åˆ†é¡çµæœåˆ° AUTHOR_CLASSIFY.json / AUTHOR_UNKNOWN.jsonã€‚
    æœƒè‡ªå‹•æ’é™¤é‡è¤‡é …ç›®ã€‚
    
    åƒæ•¸:
    - authors: List[dict]ï¼Œæ¯ç­†ç‚º {"name": ..., "tag": ...}
    """
    
    classify_data = load_author_json()
    unknown_data  = load_author_json(AUTHOR_UNKNOWN_PATH)

    # ç”¨ set åŠ é€Ÿæ¯”å°å·²å­˜åœ¨è³‡æ–™ï¼ˆåªæ¯”å° nameï¼‰
    existing_classify = { item["name"] for item in classify_data }
    existing_unknown  = { item["name"] for item in unknown_data }

    new_classify = []
    new_unknown  = []

    for author in authors:
        name = author.get("name")
        tag = author.get("tag", "unknown")

        if not name:
            continue  # è·³éç„¡æ•ˆé …ç›®

        if tag == "unknown":
            if name not in existing_unknown:
                new_unknown.append({"name": name, "tag": tag})
        else:
            if name not in existing_classify:
                new_classify.append({"name": name, "tag": tag})

    # å¯«å…¥ AUTHOR_CLASSIFY æª”æ¡ˆ
    if new_classify:
        classify_data.extend(new_classify)
        with open(AUTHOR_CLASSIFY_PATH, "w", encoding="utf-8") as f:
            json.dump(classify_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²å„²å­˜ {len(new_classify)} ç­†ä½œè€…åˆ†é¡è³‡æ–™åˆ° {classify_file} ä¸­")
    
    # å¯«å…¥ AUTHOR_UNKNOWN æª”æ¡ˆ
    if new_unknown:
        unknown_data.extend(new_unknown)
        with open(AUTHOR_UNKNOWN_PATH, "w", encoding="utf-8") as f:
            json.dump(unknown_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²å„²å­˜ {len(new_unknown)} ç­†ä½œè€…åˆ†é¡è³‡æ–™åˆ° {unknown_file} ä¸­")
    
    print(f"âœ… {classify_file} / {unknown_file} å¯«å…¥å®Œæˆ")

    return

#  å°‡ AUTHOR_UNKNOWN.json çš„è³‡æ–™ï¼Œå­˜å…¥ AUTHOR_CLASSIFY ä¸­
def AUTHOR_UNKNOWN_save_to_AUTHOR_CLASSIFY():
    classify_data = load_author_json()
    unknown_data = load_author_json(AUTHOR_UNKNOWN_PATH)

    new_classify = []
    new_unknown = []
    
    for element in unknown_data:
        if not element['tag']=='unknown':
            new_classify.append(element)
        else:
            new_unknown.append(element)

    # å¯«å…¥ AUTHOR_CLASSIFY æª”æ¡ˆ
    if new_classify:
        classify_data.extend(new_classify)
        with open(AUTHOR_CLASSIFY_PATH, "w", encoding="utf-8") as f:
            json.dump(classify_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²æ–°å¢ {len(new_classify)} ç­†ä½œè€…åˆ†é¡è³‡æ–™åˆ° AUTHOR_CLASSIFY.json ä¸­")
    
    # å¯«å…¥ AUTHOR_UNKNOWN æª”æ¡ˆ
    with open(AUTHOR_UNKNOWN_PATH, "w", encoding="utf-8") as f:
        json.dump(new_unknown, f, ensure_ascii=False, indent=2)
    print(f"âœ… å·²å„²å­˜ {len(new_unknown)} ç­†ä½œè€…åˆ†é¡è³‡æ–™åˆ° AUTHOR_UNKNOWN.json ä¸­")
    
    print(f"âœ… AUTHOR_UNKNOWN_save_to_AUTHOR_CLASSIFY å¯«å…¥å®Œæˆ")

    return