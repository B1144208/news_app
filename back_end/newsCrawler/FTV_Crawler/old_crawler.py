from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import os
import json


FTV_Main_url = "https://www.ftvnews.com.tw/"

def news_crawler(url, type):

    # è¨­å®š Chrome åŸ·è¡Œé¸é …
    options = Options()
    options.add_argument("--disable-gpu")
    options.add_argument("window-size=1920x1080")

    # è¨­å®š ChromeDriver è·¯å¾‘
    driver_path = os.path.join(os.path.dirname(__file__), '..', 'chromedriver.exe')
    service = Service(executable_path=driver_path)

    # å•Ÿå‹•ç€è¦½å™¨
    driver = webdriver.Chrome(service=service, options=options)
    driver.get(url)

    # ç­‰å¾…ç¶²é åŠ è¼‰å®Œæˆ
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "View_Img")))
    # time.sleep(2)
    
    # æŠ“ HTML åŸå§‹ç¢¼
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    
    data = []

    # æ¨™é¡Œã€ä½œè€…ã€ç™¼å¸ƒæ™‚é–“
    NewsTitle = soup.find("h1", class_="text-center border-0").get_text(strip=True)
    preface = soup.find("div", id="preface")
    p_tags = preface.find_all("p")
    NewsAuthor = p_tags[0].get_text(strip=True)
    PublishDate = soup.find("span", class_="date").get_text(strip=True)
    PublishDate = PublishDate.split("ï¼š")[1]
    print("NewsTitle   : ", NewsTitle)
    print("NewsAuthor  : ", NewsAuthor)
    print("PublishDate : ", PublishDate, '\n')

    # åœ–ç‰‡(å°é¢)
    CoverImg = soup.find("figure", id="View_Img", class_="article-cover")
    img_src = CoverImg.find("img")
    img_src = img_src.get('src') if img_src else None
    print("CoverImg    : ", img_src)
    img_fig = CoverImg.find("figcaption") if img_src else None
    print("Caption     : ", img_fig.text, '\n')

    # å…§æ–‡( text + img )
    element = p_tags[1].get_text(strip=True)
    data.append({'text': element})
    content = soup.find("div", id="newscontent")
    for element in content.find_all(["p", "figure"]):
        if element.name=="p":
            if element.get_text(strip=True) and not element.find('br'):
                data.append({'text': element.get_text(strip=True)})
        elif element.name=="figure" and "fr-img-wrap" in element.get('class', []):
            img_tag = element.find('img')
            if img_tag and img_tag.get('src'):
                # å¦‚æœæœ‰åœ–ç‰‡ï¼Œå‰‡ç²å–åœ–ç‰‡çš„ src
                img_src = img_tag['src']
                
                # æŸ¥æ‰¾åœ–ç‰‡èªªæ˜ <figcaption>
                figcaption = element.find('figcaption')
                caption_text = figcaption.get_text(strip=True) if figcaption else None
                
                # å°‡åœ–ç‰‡çš„ src å’Œèªªæ˜ä¸€èµ·å„²å­˜
                data.append({'img': img_src, 'caption': caption_text})

    # data = json.dumps(data, ensure_ascii=False, indent=4)
    # print(data)

    with open("output.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


    # é—œéµå­—
    keyword = []
    news_tag = soup.find("div", class_="news-tag")
    news_tag = news_tag.find_all("li")
    for ky in news_tag:
        key = ky.find("a").text
        keyword.append(key)
    
    print(keyword)

    driver.quit()
    return


news_crawler("https://www.ftvnews.com.tw/news/detail/2025428N08M1", "ç½·å…")


def group_crawler(url, type):

    if type=="ç†±é–€" or type=="é«”è‚²" or type=="è²¡ç¶“" or type=="æ•¸ä½å°ˆé¡Œ":
        return

    # è¨­å®š Chrome åŸ·è¡Œé¸é …
    options = Options()
    options.add_argument("--disable-gpu")
    options.add_argument("window-size=1920x1080")

    # è¨­å®š ChromeDriver è·¯å¾‘
    driver_path = os.path.join(os.path.dirname(__file__), '..', 'chromedriver.exe')
    service = Service(executable_path=driver_path)

    # å•Ÿå‹•ç€è¦½å™¨
    driver = webdriver.Chrome(service=service, options=options)
    driver.get(url)

    time.sleep(2)  # â³ ç­‰ä¸€ä¸‹å…§å®¹è·‘å®Œï¼ˆé‡è¦ï¼ç¶²é æ˜¯å‹•æ…‹ JSï¼‰

    if type=="å³æ™‚":
        # è®“é é¢ä¸€ç›´æ»‘åˆ°åº•
        last_height = driver.execute_script("return document.body.scrollHeight")

        while True:
            # æ»‘åˆ°æœ€åº•
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

            # ç­‰å¾…æ–°å…§å®¹è¼‰å…¥
            time.sleep(2)

            # é‡æ–°è¨ˆç®— scroll é«˜åº¦
            new_height = driver.execute_script("return document.body.scrollHeight")

            if new_height == last_height:
                print("âœ… åˆ°åº•äº†ï¼Œåœæ­¢æ»‘å‹•")
                break

            last_height = new_height
    
    # æŠ“ HTML åŸå§‹ç¢¼
    html = driver.page_source
    driver.quit()

    soup = BeautifulSoup(html, "html.parser")

    news_item = soup.find_all("li", class_="rl-list col-12")

    
    for news in news_item:
        a_tag = news.find("a", class_="img-block")
        if a_tag:
            news_link = a_tag.get("href")
            news_link = FTV_Main_url + news_link
            news_crawler(news_link, type)
        print(news_link)

    return


    fileName = "FTV_" + type
    # å­˜æˆ html æª”æ–¹ä¾¿é™¤éŒ¯
    # if not os.path.exists(f"{fileName}.html"):
    with open(f"{fileName}.html", "w", encoding="utf-8") as f:
        f.write(html)
    print(f"âœ… å·²å„²å­˜ {fileName}.html")

    if type=="é¦–é " or type=="é«”è‚²":
        return
    

def main():

    # è¨­å®š Chrome åŸ·è¡Œé¸é …
    options = Options()
    options.add_argument("--disable-gpu")
    options.add_argument("window-size=1920x1080")

    # è¨­å®š ChromeDriver è·¯å¾‘
    driver_path = os.path.join(os.path.dirname(__file__), '..', 'chromedriver.exe')
    service = Service(executable_path=driver_path)

    # å•Ÿå‹•ç€è¦½å™¨
    driver = webdriver.Chrome(service=service, options=options)
    driver.get(FTV_Main_url)

    time.sleep(2)  # â³ ç­‰ä¸€ä¸‹å…§å®¹è·‘å®Œï¼ˆé‡è¦ï¼ç¶²é æ˜¯å‹•æ…‹ JSï¼‰

    # æŠ“ HTML åŸå§‹ç¢¼
    html = driver.page_source
    driver.quit()


    # å­˜æˆ html æª”æ–¹ä¾¿é™¤éŒ¯
    with open("FTV_Main.html", "w", encoding="utf-8") as f:
        f.write(html)
    print("âœ… å·²å„²å­˜ HTML")

    # ä½¿ç”¨ BeautifulSoup è§£æ
    soup = BeautifulSoup(html, "html.parser")

    # å°å‡ºé€£çµ
    Menulist = soup.find_all("li", class_ = "mainMenu")


    allow_href = ["http", "https"]

    for link in Menulist:
        group = link.find("a")
        group_href = group.get("href")
        group_text = group.text
        extension = group_href.split(":")[0].lower()
        if not extension in allow_href:
            group_href = FTV_Main_url + group_href
        
        group_crawler(group_href, group_text)


# main()







































"""
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import time

url = "https://www.ftvnews.com.tw/"

chrome_options = uc.ChromeOptions()
chrome_options.add_argument("--window-size=1920,1080")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument(
    "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
)


with uc.Chrome(options=chrome_options) as driver:
    driver.get(url)
    time.sleep(5)
    html = driver.page_source

# å­˜æˆ html æª”
with open("FTV_Main.html", "w", encoding="utf-8") as f:
    f.write(html)
print("âœ… HTML å·²å„²å­˜")

# åˆ†æ
soup = BeautifulSoup(html, "html.parser")
for h2 in soup.find_all("h2"):
    print("ğŸ“°", h2.get_text(strip=True))

try:
    driver.quit()
except Exception as e:
    print("âš ï¸ é—œé–‰ç€è¦½å™¨æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼ˆå¯å¿½ç•¥ï¼‰ï¼š", e)
"""
"""
import undetected_chromedriver as uc
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import time
import os

# è¨­å®š Chrome åŸ·è¡Œé¸é …
options = uc.ChromeOptions()
options.add_argument("--disable-gpu")
options.add_argument("window-size=1920x1080")

# è¨­å®š ChromeDriver è·¯å¾‘
driver_path = os.path.join(os.path.dirname(__file__), '..', 'chromedriver.exe')

# å‰µå»º ChromeDriver æœå‹™
service = Service(executable_path=driver_path)

# å•Ÿå‹•ç€è¦½å™¨
with uc.Chrome(service=service, options=options) as driver:
    driver.get("https://www.ftvnews.com.tw/")

    time.sleep(5)  # â³ ç­‰ä¸€ä¸‹å…§å®¹è·‘å®Œï¼ˆé‡è¦ï¼ç¶²é æ˜¯å‹•æ…‹ JSï¼‰

    # æŠ“ HTML åŸå§‹ç¢¼
    html = driver.page_source

    # å­˜æˆ html æª”æ–¹ä¾¿é™¤éŒ¯
    with open("FTV_Main.html", "w", encoding="utf-8") as f:
        f.write(html)
    print("âœ… å·²å„²å­˜ HTML")

    # ä½¿ç”¨ BeautifulSoup è§£æ
    soup = BeautifulSoup(html, "html.parser")

    # å˜—è©¦å°å‡ºæ‰€æœ‰ h2 æ¨™é¡Œï¼ˆæ ¹æ“šå¯¦éš›ç¶²é èª¿æ•´æ¨™ç±¤ï¼‰
    for h2 in soup.find_all("h2"):
        print("ğŸ“°", h2.get_text(strip=True))
"""
